
@article{ren_safe_2018,
	title = {Safe Feature Screening for Generalized {LASSO}},
	volume = {40},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/8118103/},
	doi = {10.1109/TPAMI.2017.2776267},
	abstract = {Solving Generalized {LASSO} ({GL}) problems is challenging, particularly when analyzing many features with a complex interacting structure. Recent developments have found effective ways to identify inactive features so that they can be removed or aggregated to reduce the problem size before applying optimization solvers for learning. However, existing methods are mostly devoted to special cases of {GL} problems with special structures for feature interactions, such as chains or trees. Developing screening rules, particularly, safe screening rules to remove or aggregate features with general interaction structures, calls for a very different screening approach for {GL} problems. To tackle this challenge, we formulate the {GL} screening problem as a bound estimation problem in a large linear inequality system when solving them in the dual space. We propose a novel bound propagation algorithm for efﬁcient safe screening for general {GL} problems, which can be further enhanced by developing novel transformation methods that can effectively decouple interactions among features. The proposed propagation and transformation methods are applicable with dynamic screening that can easily initiate the screening process while existing screening methods require the knowledge of the solution under a desirable regularization parameter. Experiments on both synthetic and real-world data demonstrate the effectiveness of the proposed screening method.},
	pages = {2992--3006},
	number = {12},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Ren, Shaogang and Huang, Shuai and Ye, Jieping and Qian, Xiaoning},
	urldate = {2024-05-20},
	date = {2018-12-01},
	langid = {english},
	file = {Ren et al. - 2018 - Safe Feature Screening for Generalized LASSO.pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\DC9LJPEV\\Ren et al. - 2018 - Safe Feature Screening for Generalized LASSO.pdf:application/pdf},
}

@article{zhang_scaling_nodate,
	title = {Scaling Up Sparse Support Vector Machinesby Simultaneous Feature and Sample Reduction},
	abstract = {Sparse support vector machine ({SVM}) is a popular classiﬁcation technique that can simultaneously learn a small set of the most interpretable features and identify the support vectors. It has achieved great successes in many real-world applications. However, for large-scale problems involving a huge number of samples and extremely high-dimensional features, solving sparse {SVMs} remains challenging. By noting that sparse {SVMs} induce sparsities in both feature and sample spaces, we propose a novel approach, which is based on accurate estimations of the primal and dual optima of sparse {SVMs}, to simultaneously identify the features and samples that are guaranteed to be irrelevant to the outputs. Thus, we can remove the identiﬁed inactive samples and features from the training phase, leading to substantial savings in both the memory usage and computational cost without sacriﬁcing accuracy. To the best of our knowledge, the proposed method is the ﬁrst static feature and sample reduction method for sparse {SVM}. Experiments on both synthetic and real datasets (e.g., the kddb dataset with about 20 million samples and 30 million features) demonstrate that our approach signiﬁcantly outperforms state-of-the-art methods and the speedup gained by our approach can be orders of magnitude.},
	author = {Zhang, Weizhong and Hong, Bin and Liu, Wei and Ye, Jieping and Cai, Deng and He, Xiaofei and Wang, Jie},
	langid = {english},
	file = {Zhang et al. - Scaling Up Sparse Support Vector Machinesby Simult.pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\F6X8IZTD\\Zhang et al. - Scaling Up Sparse Support Vector Machinesby Simult.pdf:application/pdf},
}

@article{zhao_safe_2017,
	title = {A safe sample screening rule for Universum support vector machines},
	volume = {138},
	issn = {09507051},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705117304501},
	doi = {10.1016/j.knosys.2017.09.031},
	pages = {46--57},
	journaltitle = {Knowledge-Based Systems},
	shortjournal = {Knowledge-Based Systems},
	author = {Zhao, Jiang and Xu, Yitian},
	urldate = {2024-05-20},
	date = {2017-12},
	langid = {english},
	file = {Zhao and Xu - 2017 - A safe sample screening rule for Universum support.pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\9KCCQWHY\\Zhao and Xu - 2017 - A safe sample screening rule for Universum support.pdf:application/pdf},
}

@article{pang_doubly_2021,
	title = {A Doubly Sparse Multiclass Support Vector Machine With Simultaneous Feature and Sample Screening},
	volume = {51},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {2168-2216, 2168-2232},
	url = {https://ieeexplore.ieee.org/document/8966593/},
	doi = {10.1109/TSMC.2020.2964718},
	abstract = {{KSVCR} is an effective algorithm to handle multiclass problems. But it cannot do variable selection and is time-consuming on large datasets. In this article, we propose a doubly sparse multiclass model {DKSVCR} which employs elastic net regularization term to improve the performance of {KSVCR}. And then, motivated by the sparsity of {DKSVCR}, a simultaneous safe feature and sample screening rule {MFSS} is further constructed to accelerate the solving speed of {DKSVCR}, which is termed as {MFSS}-{DKSVCR}. It has two major beneﬁts. First, both classiﬁcation and variable selection could be realized, the highly correlated features tend to be selected or removed together. Second, by using feature screening and sample screening rule alternatively rather than using each of them individually, our {MFSS}-{DKSVCR} can delete more redundant features and samples before the training stage. Hence, the solving speed is improved a lot. And our {MFSS}-{DKSVCR} is safe in the sense that the solutions obtained from the reduced problem and original problem are identical. Besides, a fast algorithm {SDCA} is used to solve the problem more efﬁciently. The experimental results on one artiﬁcial dataset, 28 benchmark datasets, and an image dataset verify the validity of our method.},
	pages = {6911--6925},
	number = {11},
	journaltitle = {{IEEE} Transactions on Systems, Man, and Cybernetics: Systems},
	shortjournal = {{IEEE} Trans. Syst. Man Cybern, Syst.},
	author = {Pang, Xinying and Xu, Yitian and Xiao, Xinshuang},
	urldate = {2024-05-20},
	date = {2021-11},
	langid = {english},
	file = {Pang et al. - 2021 - A Doubly Sparse Multiclass Support Vector Machine .pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\FB6QIS48\\Pang et al. - 2021 - A Doubly Sparse Multiclass Support Vector Machine .pdf:application/pdf},
}

@article{folberth_safe_2020,
	title = {Safe Feature Elimination for Non-Negativity Constrained Convex Optimization},
	volume = {184},
	issn = {0022-3239, 1573-2878},
	url = {http://arxiv.org/abs/1907.10831},
	doi = {10.1007/s10957-019-01612-w},
	abstract = {Inspired by recent work on safe feature elimination for 1-norm regularized least-squares, we develop strategies to eliminate features from convex optimization problems with non-negativity constraints. Our strategy is safe in the sense that it will only remove features/coordinates from the problem when they are guaranteed to be zero at a solution. To perform feature elimination we use an accurate, but not optimal, primal-dual feasible pair, making our methods robust and able to be used on ill-conditioned problems. We supplement our feature elimination problem with a method to construct an accurate dual feasible point from an accurate primal feasible point; this allows us to use a ﬁrst-order method to ﬁnd an accurate primal feasible point, then use that point to construct an accurate dual feasible point and perform feature elimination. Under reasonable conditions, our feature elimination strategy will eventually eliminate all zero features from the problem. As an application of our methods we show how safe feature elimination can be used to robustly certify the uniqueness of non-negative least-squares ({NNLS}) problems. We give numerical examples on a well-conditioned synthetic {NNLS} problem and a on set of 40000 extremely ill-conditioned {NNLS} problems arising in a microscopy application.},
	pages = {931--952},
	number = {3},
	journaltitle = {Journal of Optimization Theory and Applications},
	shortjournal = {J Optim Theory Appl},
	author = {Folberth, James and Becker, Stephen},
	urldate = {2024-05-20},
	date = {2020-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1907.10831 [cs, math]},
	keywords = {49N15, 90C25, 90C46, Computer Science - Computer Vision and Pattern Recognition, Mathematics - Optimization and Control},
	file = {Folberth and Becker - 2020 - Safe Feature Elimination for Non-Negativity Constr.pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\4C34BSKU\\Folberth and Becker - 2020 - Safe Feature Elimination for Non-Negativity Constr.pdf:application/pdf},
}

@article{yoshida_safe_2019,
	title = {Safe Triplet Screening for Distance Metric Learning},
	volume = {31},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/31/12/2432-2491/95615},
	doi = {10.1162/neco_a_01240},
	abstract = {Distance metric learning has been widely used to obtain the optimal distance function based on the given training data. We focus on a triplet-based loss function, which imposes a penalty such that a pair of instances in the same class is closer than a pair in different classes. However, the number of possible triplets can be quite large even for a small data set, and this considerably increases the computational cost for metric optimization. In this letter, we propose safe triplet screening that identifies triplets that can be safely removed from the optimization problem without losing the optimality. In comparison with existing safe screening studies, triplet screening is particularly significant because of the huge number of possible triplets and the semidefinite constraint in the optimization problem. We demonstrate and verify the effectiveness of our screening rules by using several benchmark data sets.},
	pages = {2432--2491},
	number = {12},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Yoshida, Tomoki and Takeuchi, Ichiro and Karasuyama, Masayuki},
	urldate = {2024-05-20},
	date = {2019-12},
	langid = {english},
	file = {Yoshida et al. - 2019 - Safe Triplet Screening for Distance Metric Learnin.pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\C9T2JGRY\\Yoshida et al. - 2019 - Safe Triplet Screening for Distance Metric Learnin.pdf:application/pdf},
}

@article{wang_simultaneous_2019,
	title = {Simultaneous Safe Feature and Sample Elimination for Sparse Support Vector Regression},
	volume = {67},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {1053-587X, 1941-0476},
	url = {https://ieeexplore.ieee.org/document/8744246/},
	doi = {10.1109/TSP.2019.2924580},
	abstract = {Sparse support vector regression ({SSVR}) is an effective regression technique. It has been successfully applied to many practical problems. However, it remains challenging to handle the large-scale problems. A nice property of {SSVR} is double sparsity in the sense that most irrelevant features and samples have no effect on the regressor. Inspired by it, we propose a simultaneous safe feature and sample screening rule based on the strong convexity of objective function ({FSSR}1) to accelerate {SSVR}, including the linear and nonlinear cases. Most inactive features and samples can be simultaneously discarded before training {SSVR}. Only one reduced {SSVR} ({RSSVR}) needs to be solved. Moreover, to further speed up {RSSVR}, the screening rule based on duality gap ({FSSR}2) is applied to continuously discard the remaining inactive variables during the reduced model training process. Combining the rule with the gridsearch method, the framework of our ﬁnal method ({FSSR}–{SSVR}) is to alternatively conduct {FSSR}1 and {FSSR}2, which leads to substantial savings in both the memory usage and the computational cost. There are two appealing advantages of our {FSSR}-{SSVR}: ﬁrst, it is safe, i.e., the features and samples discarded by {FSSR} are guaranteed to be inactive; second, it has synergy effect (in other words, the results of the previous feature screening can improve the performance of the next sample screening, and vice versa). Furthermore, the stochastic dual coordinate ascent ({SDCA}) method is employed as an efﬁcient solver. Experiments on three synthetic datasets and 11 real-world datasets demonstrate the efﬁciency of our method.},
	pages = {4043--4054},
	number = {15},
	journaltitle = {{IEEE} Transactions on Signal Processing},
	shortjournal = {{IEEE} Trans. Signal Process.},
	author = {Wang, Hongmei and Pan, Xianli and Xu, Yitian},
	urldate = {2024-05-20},
	date = {2019-08-01},
	langid = {english},
	file = {Wang et al. - 2019 - Simultaneous Safe Feature and Sample Elimination f.pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\A8HE8UX5\\Wang et al. - 2019 - Simultaneous Safe Feature and Sample Elimination f.pdf:application/pdf},
}

@article{zhai_safe_2020,
	title = {Safe Sample Screening for Robust Support Vector Machine},
	volume = {34},
	rights = {https://www.aaai.org},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6182},
	doi = {10.1609/aaai.v34i04.6182},
	abstract = {Robust support vector machine ({RSVM}) has been shown to perform remarkably well to improve the generalization performance of support vector machine under the noisy environment. Unfortunately, in order to handle the non-convexity induced by ramp loss in {RSVM}, existing {RSVM} solvers often adopt the {DC} programming framework which is computationally inefﬁcient for running multiple outer loops. This hinders the application of {RSVM} to large-scale problems. Safe sample screening that allows for the exclusion of training samples prior to or early in the training process is an effective method to greatly reduce computational time. However, existing safe sample screening algorithms are limited to convex optimization problems while {RSVM} is a non-convex problem. To address this challenge, in this paper, we propose two safe sample screening rules for {RSVM} based on the framework of concave-convex procedure ({CCCP}). Speciﬁcally, we provide screening rule for the inner solver of {CCCP} and another rule for propagating screened samples between two successive solvers of {CCCP}. To the best of our knowledge, this is the ﬁrst work of safe sample screening to a non-convex optimization problem. More importantly, we provide the security guarantee to our sample screening rules to {RSVM}. Experimental results on a variety of benchmark datasets verify that our safe sample screening rules can signiﬁcantly reduce the computational time.},
	pages = {6981--6988},
	number = {4},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	shortjournal = {{AAAI}},
	author = {Zhai, Zhou and Gu, Bin and Li, Xiang and Huang, Heng},
	urldate = {2024-05-20},
	date = {2020-04-03},
	langid = {english},
	file = {Zhai et al. - 2020 - Safe Sample Screening for Robust Support Vector Ma.pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\BKXY2PGU\\Zhai et al. - 2020 - Safe Sample Screening for Robust Support Vector Ma.pdf:application/pdf},
}

@misc{shang_safe_2024,
	title = {Safe Feature Identification Rule for Fused Lasso by An Extra Dual Variable},
	url = {http://arxiv.org/abs/2404.10262},
	abstract = {Fused Lasso was proposed to characterize the sparsity of the coefficients and the sparsity of their successive differences for the linear regression. Due to its wide applications, there are many existing algorithms to solve fused Lasso. However, the computation of this model is time-consuming in high-dimensional data sets. To accelerate the calculation of fused Lasso in high-dimension data sets, we build up the safe feature identification rule by introducing an extra dual variable. With a low computational cost, this rule can eliminate inactive features with zero coefficients and identify adjacent features with same coefficients in the solution. To the best of our knowledge, existing screening rules can not be applied to speed up the computation of fused Lasso and our work is the first one to deal with this problem. To emphasize our rule is a unique result that is capable of identifying adjacent features with same coefficients, we name the result as the safe feature identification rule. Numerical experiments on simulation and real data illustrate the efficiency of the rule, which means this rule can reduce the computational time of fused Lasso. In addition, our rule can be embedded into any efficient algorithm and speed up the computational process of fused Lasso.},
	number = {{arXiv}:2404.10262},
	publisher = {{arXiv}},
	author = {Shang, Pan and Chen, Huangyue and Kong, Lingchen},
	urldate = {2024-05-20},
	date = {2024-04-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2404.10262 [math, stat]},
	keywords = {Mathematics - Optimization and Control, Statistics - Computation},
	file = {Shang et al. - 2024 - Safe Feature Identification Rule for Fused Lasso b.pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\FZCCY4DA\\Shang et al. - 2024 - Safe Feature Identification Rule for Fused Lasso b.pdf:application/pdf},
}

@article{ogawa_safe_nodate,
	title = {Safe Screening of Non-Support Vectors in Pathwise {SVM} Computation},
	abstract = {In this paper, we claim that some of the nonsupport vectors (non-{SVs}) that have no inﬂuence on the {SVM} classiﬁer can be screened out prior to the training phase in pathwise {SVM} computation scenario, in which one is asked to train a sequence (or path) of {SVM} classiﬁers for diﬀerent regularization parameters. Based on a recently proposed framework so-called safe screening rule, we derive a rule for screening out non-{SVs} in advance, and discuss how we can exploit the advantage of the rule in pathwise {SVM} computation scenario. Experiments indicate that our approach often substantially reduce the total pathwise computation cost.},
	author = {Ogawa, Kohei and Suzuki, Yoshiki and Takeuchi, Ichiro},
	langid = {english},
	file = {Ogawa et al. - Safe Screening of Non-Support Vectors in Pathwise .pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\FMRP9W42\\Ogawa et al. - Safe Screening of Non-Support Vectors in Pathwise .pdf:application/pdf},
    date = {2013},
}

@article{herzet_region-free_nodate,
	title = {Region-free Safe Screening Tests for l1-penalized Convex Problems},
	abstract = {We address the problem of safe screening for ℓ1penalized convex regression/classification problems, i.e., the identification of zero coordinates of the solutions. Unlike previous contributions of the literature, we propose a screening methodology which does not require the knowledge of a so-called “safe region”. Our approach does not rely on any other assumption than convexity (in particular, no strong-convexity hypothesis is needed) and therefore applies to a wide family of convex problems. When the Fenchel conjugate of the data-fidelity term is strongly convex, we show that the popular “{GAP} sphere test” proposed by Fercoq et al. can be recovered as a particular case of our methodology (up to a minor modification). We illustrate numerically the performance of our procedure on the “sparse support vector machine classification” problem.},
	author = {Herzet, Cédric and Elvira, Clément and Dang, Hong-Phuong},
	langid = {english},
	file = {Herzet et al. - Region-free Safe Screening Tests for l1-penalized .pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\L59ZRHJY\\Herzet et al. - Region-free Safe Screening Tests for l1-penalized .pdf:application/pdf},
}

@article{tanveer_comprehensive_2022,
	title = {Comprehensive Review On Twin Support Vector Machines},
	issn = {0254-5330, 1572-9338},
	url = {http://arxiv.org/abs/2105.00336},
	doi = {10.1007/s10479-022-04575-w},
	abstract = {Twin support vector machine ({TWSVM}) and twin support vector regression ({TSVR}) are newly emerging eﬃcient machine learning techniques which oﬀer promising solutions for classiﬁcation and regression challenges respectively. {TWSVM} is based upon the idea to identify two nonparallel hyperplanes which classify the data points to their respective classes. It requires to solve two small sized quadratic programming problems ({QPPs}) in lieu of solving single large size {QPP} in support vector machine ({SVM}) while {TSVR} is formulated on the lines of {TWSVM} and requires to solve two {SVM} kind problems. Although there has been good research progress on these techniques; there is limited literature on the comparison of diﬀerent variants of {TSVR}. Thus, this review presents a rigorous analysis of recent research in {TWSVM} and {TSVR} simultaneously mentioning their limitations and advantages. To begin with, we ﬁrst introduce the basic theory of support vector machine, {TWSVM} and then focus on the various improvements and applications of {TWSVM}, and then we introduce {TSVR} and its various enhancements. Finally, we suggest future research and development prospects.},
	journaltitle = {Annals of Operations Research},
	shortjournal = {Ann Oper Res},
	author = {Tanveer, M. and Rajani, T. and Rastogi, R. and Shao, Y. H. and Ganaie, M. A.},
	urldate = {2024-05-20},
	date = {2022-03-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2105.00336 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Tanveer et al. - 2022 - Comprehensive Review On Twin Support Vector Machin.pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\DKR9ANAC\\Tanveer et al. - 2022 - Comprehensive Review On Twin Support Vector Machin.pdf:application/pdf},
}

@article{pan_safe_2018,
	title = {Safe Screening Rules for Accelerating Twin Support Vector Machine Classification},
	volume = {29},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {2162-237X, 2162-2388},
	url = {http://ieeexplore.ieee.org/document/7896625/},
	doi = {10.1109/TNNLS.2017.2688182},
	abstract = {The twin support vector machine ({TSVM}) is widely used in classiﬁcation problems, but it is not efﬁcient enough for large-scale data sets. Furthermore, to get the optimal parameter, the exhaustive grid search method is applied to {TSVM}. It is very time-consuming, especially for multiparameter models. Although many techniques have been presented to solve these problems, all of them always affect the performance of {TSVM} to some extent. In this paper, we propose a safe screening rule ({SSR}) for {linearTSVM}, and give a modiﬁed {SSR} ({MSSR}) for nonlinear {TSVM}, which contains multiple parameters. The {SSR} and {MSSR} can delete most training samples and reduce the scale of {TSVM} before solving it. Sequential versions of {SSR} and {MSSR} are further introduced to substantially accelerate the whole parameter tuning process. One important advantage of {SSR} and {MSSR} is that they are safe, i.e., we can obtain the same solution as the original problem by utilizing them. Experiments on eight real-world data sets and an imbalanced data set with different imbalanced ratios demonstrate the efﬁciency and safety of {SSR} and {MSSR}.},
	pages = {1876--1887},
	number = {5},
	journaltitle = {{IEEE} Transactions on Neural Networks and Learning Systems},
	shortjournal = {{IEEE} Trans. Neural Netw. Learning Syst.},
	author = {Pan, Xianli and Yang, Zhiji and Xu, Yitian and Wang, Laisheng},
	urldate = {2024-05-20},
	date = {2018-05},
	langid = {english},
	file = {Pan et al. - 2018 - Safe Screening Rules for Accelerating Twin Support.pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\GPACFN92\\Pan et al. - 2018 - Safe Screening Rules for Accelerating Twin Support.pdf:application/pdf},
}

@article{wang_scaling_nodate,
	title = {Scaling {SVM} and Least Absolute Deviations via Exact Data Reduction},
	abstract = {The support vector machine ({SVM}) is a widely used method for classiﬁcation. Although many efforts have been devoted to develop efﬁcient solvers, it remains challenging to apply {SVM} to large-scale problems. A nice property of {SVM} is that the non-support vectors have no effect on the resulting classiﬁer. Motivated by this observation, we present fast and efﬁcient screening rules to discard non-support vectors by analyzing the dual problem of {SVM} via variational inequalities ({DVI}). As a result, the number of data instances to be entered into the optimization can be substantially reduced. Some appealing features of our screening method are: (1) {DVI} is safe in the sense that the vectors discarded by {DVI} are guaranteed to be non-support vectors; (2) the data set needs to be scanned only once to run the screening, and its computational cost is negligible compared to that of solving the {SVM} problem; (3) {DVI} is independent of the solvers and can be integrated with any existing efﬁcient solver. We also show that the {DVI} technique can be extended to detect non-support vectors in the least absolute deviations regression ({LAD}). To the best of our knowledge, there are currently no screening methods for {LAD}. We have evaluated {DVI} on both synthetic and real data sets. Experiments indicate that {DVI} signiﬁcantly outperforms the existing state-of-the-art screening rules for {SVM}, and it is very effective in discarding non-support vectors for {LAD}. The speedup gained by {DVI} rules can be up to two orders of magnitude.},
	author = {Wang, Jie and Wonka, Peter and Ye, Jieping},
	langid = {english},
	file = {Wang et al. - Scaling SVM and Least Absolute Deviations via Exac.pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\AS4GAF93\\Wang et al. - Scaling SVM and Least Absolute Deviations via Exac.pdf:application/pdf},
}

@misc{su_safe_2023,
	title = {Safe Screening for Unbalanced Optimal Transport},
	url = {http://arxiv.org/abs/2307.00247},
	abstract = {This paper introduces a framework that utilizes the Safe Screening technique to accelerate the optimization process of the Unbalanced Optimal Transport ({UOT}) problem by proactively identifying and eliminating zero elements in the sparse solutions. We demonstrate the feasibility of applying Safe Screening to the {UOT} problem with ℓ2-penalty and {KL}-penalty by conducting an analysis of the solution’s bounds and considering the local strong convexity of the dual problem. Considering the specific structural characteristics of the {UOT} in comparison to general Lasso problems on the index matrix, we specifically propose a novel approximate projection, an elliptical safe region construction, and a two-hyperplane relaxation method. These enhancements significantly improve the screening efficiency for the {UOT}’s without altering the algorithm’s complexity.},
	number = {{arXiv}:2307.00247},
	publisher = {{arXiv}},
	author = {Su, Xun and Fang, Zhongxi and Kasai, Hiroyuki},
	urldate = {2024-05-20},
	date = {2023-07-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2307.00247 [cs, math]},
	keywords = {Mathematics - Optimization and Control, Computer Science - Machine Learning},
	file = {Su et al. - 2023 - Safe Screening for Unbalanced Optimal Transport.pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\RZ4PA37S\\Su et al. - 2023 - Safe Screening for Unbalanced Optimal Transport.pdf:application/pdf},
}

@inproceedings{hsieh_dual_2008,
	location = {Helsinki, Finland},
	title = {A dual coordinate descent method for large-scale linear {SVM}},
	isbn = {978-1-60558-205-4},
	url = {http://portal.acm.org/citation.cfm?doid=1390156.1390208},
	doi = {10.1145/1390156.1390208},
	abstract = {In many applications, data appear with a huge number of instances as well as features. Linear Support Vector Machines ({SVM}) is one of the most popular tools to deal with such large-scale sparse data. This paper presents a novel dual coordinate descent method for linear {SVM} with L1- and L2loss functions. The proposed method is simple and reaches an -accurate solution in O(log(1/ )) iterations. Experiments indicate that our method is much faster than state of the art solvers such as Pegasos, {TRON}, {SVMperf} , and a recent primal coordinate descent implementation.},
	eventtitle = {the 25th international conference},
	pages = {408--415},
	booktitle = {Proceedings of the 25th international conference on Machine learning - {ICML} '08},
	publisher = {{ACM} Press},
	author = {Hsieh, Cho-Jui and Chang, Kai-Wei and Lin, Chih-Jen and Keerthi, S. Sathiya and Sundararajan, S.},
	urldate = {2024-05-20},
	date = {2008},
	langid = {english},
	file = {Hsieh et al. - 2008 - A dual coordinate descent method for large-scale l.pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\A4CB9E92\\Hsieh et al. - 2008 - A dual coordinate descent method for large-scale l.pdf:application/pdf},
}

@misc{ghaoui_safe_2011,
	title = {Safe Feature Elimination for the {LASSO} and Sparse Supervised Learning Problems},
	url = {http://arxiv.org/abs/1009.4219},
	abstract = {We describe a fast method to eliminate features (variables) in l1-penalized least-square regression (or {LASSO}) problems. The elimination of features leads to a potentially substantial reduction in running time, especially for large values of the penalty parameter. Our method is not heuristic: it only eliminates features that are guaranteed to be absent after solving the {LASSO} problem. The feature elimination step is easy to parallelize and can test each feature for elimination independently. Moreover, the computational eﬀort of our method is negligible compared to that of solving the {LASSO} problem - roughly it is the same as single gradient step. Our method extends the scope of existing {LASSO} algorithms to treat larger data sets, previously out of their reach. We show how our method can be extended to general l1-penalized convex problems and present preliminary results for the Sparse Support Vector Machine and Logistic Regression problems.},
	number = {{arXiv}:1009.4219},
	publisher = {{arXiv}},
	author = {Ghaoui, Laurent El and Viallon, Vivian and Rabbani, Tarek},
	urldate = {2024-05-20},
	date = {2011-05-18},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1009.4219 [cs, math]},
	keywords = {Mathematics - Optimization and Control, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control},
	file = {Ghaoui et al. - 2011 - Safe Feature Elimination for the LASSO and Sparse .pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\LZ8IW2G5\\Ghaoui et al. - 2011 - Safe Feature Elimination for the LASSO and Sparse .pdf:application/pdf},
}

@article{wang_l_2013,
	title = {The L 1 penalized {LAD} estimator for high dimensional linear regression},
	volume = {120},
	issn = {0047259X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0047259X1300047X},
	doi = {10.1016/j.jmva.2013.04.001},
	pages = {135--151},
	journaltitle = {Journal of Multivariate Analysis},
	shortjournal = {Journal of Multivariate Analysis},
	author = {Wang, Lie},
	urldate = {2024-05-20},
	date = {2013-09},
	langid = {english},
	file = {Wang - 2013 - The L 1 penalized LAD estimator for high dimension.pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\RIF9NDYS\\Wang - 2013 - The L 1 penalized LAD estimator for high dimension.pdf:application/pdf},
}

@article{zimmert_safe_nodate,
	title = {Safe screening for support vector machines},
	abstract = {The L2-regularized hinge loss kernel {SVM} is one of the most important and most studied machine learning algorithms. Unfortunately, its computational training time complexity is generally unsuitable for big data. Empirical runtimes can however often be reduced using shrinking heuristics on the training sample set, which exploit the fact that non-support vectors do not affect the decision boundary. These shrinking heuristics are neither well understood nor especially reliable. We present the ﬁrst safe removal bound for data points which does not rely on spectral properties of the kernel matrix. From there a relaxation provides us with a shrinking heuristic that is more reliable and performs favorably compared to a state-of-the-art shrinking heuristic suggested by Joachims [1], opening up an opportunity to improve the state of the art.},
	author = {Zimmert, Julian and de Witt, Christian Schroeder and Kerg, Giancarlo and Kloft, Marius},
	langid = {english},
	file = {Zimmert et al. - Safe screening for support vector machines.pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\NR4YVGQB\\Zimmert et al. - Safe screening for support vector machines.pdf:application/pdf},
}

@misc{shibagaki_simultaneous_2016,
	title = {Simultaneous Safe Screening of Features and Samples in Doubly Sparse Modeling},
	url = {http://arxiv.org/abs/1602.02485},
	abstract = {The problem of learning a sparse model is conceptually interpreted as the process of identifying active features/samples and then optimizing the model over them. Recently introduced safe screening allows us to identify a part of non-active features/samples. So far, safe screening has been individually studied either for feature screening or for sample screening. In this paper, we introduce a new approach for safely screening features and samples simultaneously by alternatively iterating feature and sample screening steps. A signiﬁcant advantage of considering them simultaneously rather than individually is that they have a synergy eﬀect in the sense that the results of the previous safe feature screening can be exploited for improving the next safe sample screening performances, and vice-versa. We ﬁrst theoretically investigate the synergy eﬀect, and then illustrate the practical advantage through intensive numerical experiments for problems with large numbers of features and samples.},
	number = {{arXiv}:1602.02485},
	publisher = {{arXiv}},
	author = {Shibagaki, Atsushi and Karasuyama, Masayuki and Hatano, Kohei and Takeuchi, Ichiro},
	urldate = {2024-05-20},
	date = {2016-02-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1602.02485 [stat]},
	keywords = {appendix},
	file = {Shibagaki et al. - 2016 - Simultaneous Safe Screening of Features and Sample.pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\VSYIAW3K\\Shibagaki et al. - 2016 - Simultaneous Safe Screening of Features and Sample.pdf:application/pdf},
}

@article{shibagaki_simultaneous_nodate,
	title = {Simultaneous Safe Screening of Features and Samples in Doubly Sparse Modeling},
	abstract = {The problem of learning a sparse model is conceptually interpreted as the process of identifying active features/samples and then optimizing the model over them. Recently introduced safe screening allows us to identify a part of nonactive features/samples. So far, safe screening has been individually studied either for feature screening or for sample screening. In this paper, we introduce a new approach for safely screening features and samples simultaneously by alternatively iterating feature and sample screening steps. A signiﬁcant advantage of considering them simultaneously rather than individually is that they have a synergy effect in the sense that the results of the previous safe feature screening can be exploited for improving the next safe sample screening performances, and viceversa. We ﬁrst theoretically investigate the synergy effect, and then illustrate the practical advantage through intensive numerical experiments for problems with large numbers of features and samples.},
	author = {Shibagaki, Atsushi and Karasuyama, Masayuki and Hatano, Kohei and Takeuchi, Ichiro and Nit, Shibagaki A Mllab and Jp, Nitech Ac and Jp, Inf Kyushu-U Ac and Ichiro, Takeuchi and Jp, Nitech Ac},
	langid = {english},
	file = {Shibagaki et al. - Simultaneous Safe Screening of Features and Sample.pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\7U6J5WMI\\Shibagaki et al. - Simultaneous Safe Screening of Features and Sample.pdf:application/pdf},
}

@misc{tran_one_2023,
	title = {One to beat them all: "{RYU}'' -- a unifying framework for the construction of safe balls},
	url = {http://arxiv.org/abs/2312.00640},
	shorttitle = {One to beat them all},
	abstract = {In this paper, we put forth a novel framework (named “{RYU}”) for the construction of “safe” balls, i.e., regions that provably contain the dual solution of a target optimization problem. We concentrate on the standard setup where the cost function is the sum of two terms: a closed, proper, convex Lipschitz-smooth function and a closed, proper, convex function. The {RYU} framework is shown to generalize or improve upon all the results proposed in the last decade for the considered family of optimization problems.},
	number = {{arXiv}:2312.00640},
	publisher = {{arXiv}},
	author = {Tran, Thu-Le and Elvira, Clément and Dang, Hong-Phuong and Herzet, Cédric},
	urldate = {2024-05-20},
	date = {2023-12-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2312.00640 [cs, math, stat]},
	file = {Tran et al. - 2023 - One to beat them all RYU'' -- a unifying framewo.pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\M4AVS9JT\\Tran et al. - 2023 - One to beat them all RYU'' -- a unifying framewo.pdf:application/pdf},
}

@article{fercoq_mind_nodate,
	title = {Mind the duality gap: safer rules for the Lasso},
	abstract = {Screening rules allow to early discard irrelevant variables from the optimization in Lasso problems, or its derivatives, making solvers faster. In this paper, we propose new versions of the socalled safe rules for the Lasso. Based on duality gap considerations, our new rules create safe test regions whose diameters converge to zero, provided that one relies on a converging solver. This property helps screening out more variables, for a wider range of regularization parameter values. In addition to faster convergence, we prove that we correctly identify the active sets (supports) of the solutions in ﬁnite time. While our proposed strategy can cope with any solver, its performance is demonstrated using a coordinate descent algorithm particularly adapted to machine learning use cases. Signiﬁcant computing time reductions are obtained with respect to previous safe rules.},
	author = {Fercoq, Olivier and Gramfort, Alexandre and Salmon, Joseph},
	langid = {english},
	file = {Fercoq et al. - Mind the duality gap safer rules for the Lasso.pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\BHP7TX3G\\Fercoq et al. - Mind the duality gap safer rules for the Lasso.pdf:application/pdf},
}

@article{ndiaye_gap_nodate,
	title = {Gap Safe Screening Rules for Sparsity Enforcing Penalties},
	abstract = {In high dimensional regression settings, sparsity enforcing penalties have proved useful to regularize the data-ﬁtting term. A recently introduced technique called screening rules propose to ignore some variables in the optimization leveraging the expected sparsity of the solutions and consequently leading to faster solvers. When the procedure is guaranteed not to discard variables wrongly the rules are said to be safe. In this work, we propose a unifying framework for generalized linear models regularized with standard sparsity enforcing penalties such as 1 or 1\{ 2 norms. Our technique allows to discard safely more variables than previously considered safe rules, particularly for low regularization parameters. Our proposed Gap Safe rules (so called because they rely on duality gap computation) can cope with any iterative solver but are particularly well suited to (block) coordinate descent methods. Applied to many standard learning tasks, Lasso, Sparse-Group Lasso, multitask Lasso, binary and multinomial logistic regression, etc., we report signiﬁcant speed-ups compared to previously proposed safe rules on all tested data sets.},
	author = {Ndiaye, Eugene and Fercoq, Olivier and Gramfort, Alexandre},
	langid = {english},
	file = {Ndiaye et al. - Gap Safe Screening Rules for Sparsity Enforcing Pe.pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\5F2DQ65K\\Ndiaye et al. - Gap Safe Screening Rules for Sparsity Enforcing Pe.pdf:application/pdf},
}

@inproceedings{ndiaye_gap_2016,
	title = {{GAP} Safe Screening Rules for Sparse-Group Lasso},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/hash/555d6702c950ecb729a966504af0a635-Abstract.html},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Ndiaye, Eugene and Fercoq, Olivier and Gramfort, Alexandre and Salmon, Joseph},
	urldate = {2024-05-20},
	date = {2016},
	file = {Full Text PDF:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\CFQR5AAS\\Ndiaye et al. - 2016 - GAP Safe Screening Rules for Sparse-Group Lasso.pdf:application/pdf},
}

@inproceedings{ndiaye_gap_2015,
	title = {{GAP} Safe screening rules for sparse multi-task and multi-class models},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper_files/paper/2015/hash/69421f032498c97020180038fddb8e24-Abstract.html},
	abstract = {High dimensional regression benefits from sparsity promoting regularizations. Screening rules leverage the known sparsity of the solution by ignoring some variables in the optimization, hence speeding up solvers. When the procedure is proven not to discard features wrongly the rules are said to be safe. In this paper we derive new safe rules for generalized linear models regularized with L1 and L1/L2 norms. The rules are based on duality gap computations and spherical safe regions whose diameters converge to zero. This allows to discard safely more variables, in particular for low regularization parameters. The {GAP} Safe rule can cope with any iterative solver and we illustrate its performance on coordinate descent for multi-task Lasso, binary and multinomial logistic regression, demonstrating significant speed ups on all tested datasets with respect to previous safe rules.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Ndiaye, Eugene and Fercoq, Olivier and Gramfort, Alexandre and Salmon, Joseph},
	urldate = {2024-05-20},
	date = {2015},
	file = {Full Text PDF:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\NK3S7FJ9\\Ndiaye et al. - 2015 - GAP Safe screening rules for sparse multi-task and.pdf:application/pdf},
}

@article{dantas_expanding_nodate,
	title = {Expanding Boundaries of Gap Safe Screening},
	abstract = {Sparse optimization problems are ubiquitous in many ﬁelds such as statistics, signal/image processing and machine learning. This has led to the birth of many iterative algorithms to solve them. A powerful strategy to boost the performance of these algorithms is known as safe screening: it allows the early identiﬁcation of zero coordinates in the solution, which can then be eliminated to reduce the problem’s size and accelerate convergence. In this work, we extend the existing Gap Safe screening framework by relaxing the global strong-concavity assumption on the dual cost function. Instead, we exploit local regularity properties, that is, strong concavity on well-chosen subsets of the domain. The non-negativity constraint is also integrated to the existing framework. Besides making safe screening possible to a broader class of functions that includes β-divergences (e.g., the Kullback-Leibler divergence), the proposed approach also improves upon the existing Gap Safe screening rules on previously applicable cases (e.g., logistic regression). The proposed general framework is exempliﬁed by some notable particular cases: logistic function, β = 1.5 and Kullback-Leibler divergences. Finally, we showcase the eﬀectiveness of the proposed screening rules with diﬀerent solvers (coordinate descent, multiplicative-update and proximal gradient algorithms) and diﬀerent data sets (binary classiﬁcation, hyperspectral and count data).},
	author = {Dantas, Cassio F and Soubies, Emmanuel and Févotte, Cédric},
	langid = {english},
	file = {Dantas et al. - Expanding Boundaries of Gap Safe Screening.pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\4QVMGI7I\\Dantas et al. - Expanding Boundaries of Gap Safe Screening.pdf:application/pdf},
}

@inproceedings{bao_fast_2020,
	title = {Fast {OSCAR} and {OWL} Regression via Safe Screening Rules},
	url = {https://proceedings.mlr.press/v119/bao20b.html},
	abstract = {Ordered Weighted \$L\_\{1\}\$ ({OWL}) regularized regression is a new regression analysis for high-dimensional sparse learning. Proximal gradient methods are used as standard approaches to solve {OWL} regression. However, it is still a burning issue to solve {OWL} regression due to considerable computational cost and memory usage when the feature or sample size is large. In this paper, we propose the first safe screening rule for {OWL} regression by exploring the order of the primal solution with the unknown order structure via an iterative strategy, which overcomes the difficulties of tackling the non-separable regularizer. It effectively avoids the updates of the parameters whose coefficients must be zero during the learning process. More importantly, the proposed screening rule can be easily applied to standard and stochastic proximal gradient methods. Moreover, we prove that the algorithms with our screening rule are guaranteed to have identical results with the original algorithms. Experimental results on a variety of datasets show that our screening rule leads to a significant computational gain without any loss of accuracy, compared to existing competitive algorithms.},
	eventtitle = {International Conference on Machine Learning},
	pages = {653--663},
	booktitle = {Proceedings of the 37th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Bao, Runxue and Gu, Bin and Huang, Heng},
	urldate = {2024-05-20},
	date = {2020-11-21},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\4LZJI56Z\\Bao et al. - 2020 - Fast OSCAR and OWL Regression via Safe Screening R.pdf:application/pdf},
}

@misc{massias_safe_2017,
	title = {From safe screening rules to working sets for faster Lasso-type solvers},
	url = {http://arxiv.org/abs/1703.07285},
	abstract = {Convex sparsity-promoting regularizations are ubiquitous in modern statistical learning. By construction, they yield solutions with few non-zero coeﬃcients, which correspond to saturated constraints in the dual optimization formulation. Working set ({WS}) strategies are generic optimization techniques that consist in solving simpler problems that only consider a subset of constraints, whose indices form the {WS}. Working set methods therefore involve two nested iterations: the outer loop corresponds to the deﬁnition of the {WS} and the inner loop calls a solver for the subproblems. For the Lasso estimator a {WS} is a set of features, while for a Group Lasso it refers to a set of groups. In practice, {WS} are generally small in this context so the associated feature Gram matrix can ﬁt in memory. Here we show that the Gauss-Southwell rule (a greedy strategy for block coordinate descent techniques) leads to fast solvers in this case. Combined with a working set strategy based on an aggressive use of so-called Gap Safe screening rules, we propose a solver achieving state-of-the-art performance on sparse learning problems. Results are presented on Lasso and multi-task Lasso estimators.},
	number = {{arXiv}:1703.07285},
	publisher = {{arXiv}},
	author = {Massias, Mathurin and Gramfort, Alexandre and Salmon, Joseph},
	urldate = {2024-05-20},
	date = {2017-05-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1703.07285 [cs, math, stat]},
	keywords = {Mathematics - Optimization and Control, Statistics - Computation, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Massias et al. - 2017 - From safe screening rules to working sets for fast.pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\DTBS2E9H\\Massias et al. - 2017 - From safe screening rules to working sets for fast.pdf:application/pdf},
}

@inproceedings{massias_celer_2018,
	title = {Celer: a Fast Solver for the Lasso with Dual Extrapolation},
	url = {https://proceedings.mlr.press/v80/massias18a.html},
	shorttitle = {Celer},
	abstract = {Convex sparsity-inducing regularizations are ubiquitous in high-dimensional machine learning, but solving the resulting optimization problems can be slow. To accelerate solvers, state-of-the-art approaches consist in reducing the size of the optimization problem at hand. In the context of regression, this can be achieved either by discarding irrelevant features (screening techniques) or by prioritizing features likely to be included in the support of the solution (working set techniques). Duality comes into play at several steps in these techniques. Here, we propose an extrapolation technique starting from a sequence of iterates in the dual that leads to the construction of improved dual points. This enables a tighter control of optimality as used in stopping criterion, as well as better screening performance of Gap Safe rules. Finally, we propose a working set strategy based on an aggressive use of Gap Safe screening rules. Thanks to our new dual point construction, we show significant computational speedups on multiple real-world problems.},
	eventtitle = {International Conference on Machine Learning},
	pages = {3315--3324},
	booktitle = {Proceedings of the 35th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Massias, Mathurin and Gramfort, Alexandre and Salmon, Joseph},
	urldate = {2024-05-20},
	date = {2018-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\ZPPRZDP9\\Massias et al. - 2018 - Celer a Fast Solver for the Lasso with Dual Extra.pdf:application/pdf},
}

@inproceedings{yamada_dynamic_2021,
	title = {Dynamic Sasvi: Strong Safe Screening for Norm-Regularized Least Squares},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/7b5b23f4aadf9513306bcd59afb6e4c9-Abstract.html},
	shorttitle = {Dynamic Sasvi},
	abstract = {A recently introduced technique, called safe screening,'' for a sparse optimization problem allows us to identify irrelevant variables in the early stages of optimization. In this paper, we first propose a flexible framework for safe screening based on the Fenchel--Rockafellar duality and then derive a strong safe screening rule for norm-regularized least squares using the proposed framework. We refer to the proposed screening rule for norm-regularized least squares asdynamic Sasvi'' because it can be interpreted as a generalization of Sasvi. Unlike the original Sasvi, it does not require the exact solution of a more strongly regularized problem; hence, it works safely in practice. We show that our screening rule always eliminates more features compared with the existing state-of-the-art methods.},
	pages = {14645--14655},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Yamada, Hiroaki and Yamada, Makoto},
	urldate = {2024-05-20},
	date = {2021},
	file = {Full Text PDF:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\B3J6RPB4\\Yamada and Yamada - 2021 - Dynamic Sasvi Strong Safe Screening for Norm-Regu.pdf:application/pdf},
}

@article{liu_safe_nodate,
	title = {Safe Screening with Variational Inequalities and Its Application to Lasso},
	abstract = {Sparse learning techniques have been routinely used for feature selection as the resulting model usually has a small number of non-zero entries. Safe screening, which eliminates the features that are guaranteed to have zero coefﬁcients for a certain value of the regularization parameter, is a technique for improving the computational efﬁciency. Safe screening is gaining increasing attention since 1) solving sparse learning formulations usually has a high computational cost especially when the number of features is large and 2) one needs to try several regularization parameters to select a suitable model. In this paper, we propose an approach called “Sasvi” (Safe screening with variational inequalities). Sasvi makes use of the variational inequality that provides the sufﬁcient and necessary optimality condition for the dual problem. Several existing approaches for Lasso screening can be casted as relaxed versions of the proposed Sasvi, thus Sasvi provides a stronger safe screening rule. We further study the monotone properties of Sasvi for Lasso, based on which a sure removal regularization parameter can be identiﬁed for each feature. Experimental results on both synthetic and real data sets are reported to demonstrate the effectiveness of the proposed Sasvi for Lasso screening.},
	author = {Liu, Jun and Zhao, Zheng and Wang, Jie and Ye, Jieping},
	langid = {english},
	file = {Liu et al. - Safe Screening with Variational Inequalities and I.pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\P8AX7KD9\\Liu et al. - Safe Screening with Variational Inequalities and I.pdf:application/pdf},
}

@inproceedings{guyard_node-screening_2022,
	location = {Limoges, France},
	title = {Node-screening tests for the L0-penalized least-squares problem},
	url = {https://hal.science/hal-04278109},
	booktitle = {Journée {SMAI} {MODE} 2022},
	author = {Guyard, Théo and Herzet, Cédric and Elvira, Clément},
	urldate = {2024-05-20},
	date = {2022-05},
	keywords = {Safe screening, Branch and Bound, Mixed-integer Programming, Sparse approximation},
	file = {Full Text PDF:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\XHQVBY29\\Guyard et al. - 2022 - Node-screening tests for the L0-penalized least-sq.pdf:application/pdf},
}

@inproceedings{wang_safe_2014,
	title = {A Safe Screening Rule for Sparse Logistic Regression},
	volume = {27},
	url = {https://proceedings.neurips.cc/paper/2014/hash/185c29dc24325934ee377cfda20e414c-Abstract.html},
	abstract = {The l1-regularized logistic regression (or sparse logistic regression) is a widely used method for simultaneous classification and feature selection. Although many recent efforts have been devoted to its efficient implementation, its application to high dimensional data still poses significant challenges. In this paper, we present a fast and effective sparse logistic regression screening rule (Slores) to identify the zero components in the solution vector, which may lead to a substantial reduction in the number of features to be entered to the optimization. An appealing feature of Slores is that the data set needs to be scanned only once to run the screening and its computational cost is negligible compared to that of solving the sparse logistic regression problem. Moreover, Slores is independent of solvers for sparse logistic regression, thus Slores can be integrated with any existing solver to improve the efficiency. We have evaluated Slores using high-dimensional data sets from different applications. Extensive experimental results demonstrate that Slores outperforms the existing state-of-the-art screening rules and the efficiency of solving sparse logistic regression is improved by one magnitude in general.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Wang, Jie and Zhou, Jiayu and Liu, Jun and Wonka, Peter and Ye, Jieping},
	urldate = {2024-05-20},
	date = {2014},
	file = {Full Text PDF:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\4L2F9V8P\\Wang et al. - 2014 - A Safe Screening Rule for Sparse Logistic Regressi.pdf:application/pdf},
}

@misc{tran_beyond_2022,
	title = {Beyond {GAP} screening for Lasso by exploiting new dual cutting half-spaces with supplementary material},
	url = {http://arxiv.org/abs/2203.00987},
	abstract = {In this paper, we propose a novel safe screening test for Lasso. Our procedure is based on a safe region with a dome geometry and exploits a canonical representation of the set of half-spaces (referred to as “dual cutting half-spaces” in this paper) containing the dual feasible set. The proposed safe region is shown to be always included in the state-of-the-art “{GAP} Sphere” and “{GAP} Dome” proposed by Fercoq et al. (and strictly so under very mild conditions) while involving the same computational burden. Numerical experiments conﬁrm that our new dome enables to devise more powerful screening tests than {GAP} regions and lead to signiﬁcant acceleration to solve Lasso.},
	number = {{arXiv}:2203.00987},
	publisher = {{arXiv}},
	author = {Tran, Thu-Le and Elvira, Clément and Dang, Hong-Phuong and Herzet, Cédric},
	urldate = {2024-05-20},
	date = {2022-03-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2203.00987 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Tran et al. - 2022 - Beyond GAP screening for Lasso by exploiting new d.pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\EAZ3MSRF\\Tran et al. - 2022 - Beyond GAP screening for Lasso by exploiting new d.pdf:application/pdf},
}

@misc{shang_l1-norm_2020,
	title = {l1-norm quantile regression screening rule via the dual circumscribed sphere},
	url = {http://arxiv.org/abs/2012.09395},
	abstract = {l1-norm quantile regression is a common choice if there exists outlier or heavy-tailed error in high-dimensional data sets. However, it is computationally expensive to solve this problem when the feature size of data is ultra high. As far as we know, existing screening rules can not speed up the computation of the l1-norm quantile regression, which dues to the non-differentiability of the quantile function/pinball loss. In this paper, we introduce the dual circumscribed sphere technique and propose a novel l1-norm quantile regression screening rule. Our rule is expressed as the closed-form function of given data and eliminates inactive features with a low computational cost. Numerical experiments on some simulation and real data sets show that this screening rule can be used to eliminate almost all inactive features. Moreover, this rule can help to reduce up to 23 times of computational time, compared with the computation without our screening rule.},
	number = {{arXiv}:2012.09395},
	publisher = {{arXiv}},
	author = {Shang, Pan and Kong, Lingchen},
	urldate = {2024-05-21},
	date = {2020-12-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2012.09395 [math, stat]},
	keywords = {Mathematics - Optimization and Control, Statistics - Methodology},
	file = {Shang and Kong - 2020 - l1-norm quantile regression screening rule via the.pdf:C\:\\Users\\LENOVO-PC\\Zotero\\storage\\YNQ5IG2C\\Shang and Kong - 2020 - l1-norm quantile regression screening rule via the.pdf:application/pdf},
}
